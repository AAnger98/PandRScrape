import os
import time
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# CONFIG
START_ID = 260000
END_ID = 270000
MAX_WORKERS = 5
OUTPUT_DIR = "screenshots"
CSV_FILE = "event_data.csv"
BASE_URL = "https://www.newtown-ct.gov/parks-recreation/events/"

os.makedirs(OUTPUT_DIR, exist_ok=True)

csv_fields = ['URL', 'Event Title', 'Event Date']
csv_rows = []

def create_driver():
    options = Options()
    options.headless = True
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)")
    return webdriver.Chrome(options=options)

def scrape_event(event_id):
    url = f"{BASE_URL}{event_id}"
    try:
        driver = create_driver()
        driver.set_window_size(1200, 800)
        driver.get(url)
        time.sleep(3)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # Use actual page header (not <title>) as the event name
        h1 = soup.select_one('h1.page-title')
        event_title = h1.get_text(strip=True) if h1 else ""

        # If there's no title or it's generic, skip it
        if not event_title or event_title.lower() in ["address unknown", "newtownct"]:
            print(f"[-] Skipping {url} | Invalid content")
            driver.quit()
            return

        # Try to extract a date if available
        date = ""
        date_elem = soup.select_one('.field--name-field-date')
        if date_elem:
            date = date_elem.get_text(strip=True)

        # Save screenshot
        screenshot_path = os.path.join(OUTPUT_DIR, f"{event_id}.png")
        driver.save_screenshot(screenshot_path)
        driver.quit()

        print(f"[+] Found: {url} | {event_title} | {date}")
        return [url, event_title, date]

    except Exception as e:
        print(f"[!] Error with {url}: {e}")
        return

# Run concurrently
with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    futures = [executor.submit(scrape_event, i) for i in range(START_ID, END_ID)]
    for future in as_completed(futures):
        row = future.result()
        if row:
            csv_rows.append(row)

# Save CSV
with open(CSV_FILE, 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(csv_fields)
    writer.writerows(csv_rows)

print(f"\nâœ… Done! Saved {len(csv_rows)} valid events to {CSV_FILE}")
